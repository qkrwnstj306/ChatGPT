{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f599f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "model = 'gpt-3.5-turbo'\n",
    "region = 'network'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0815124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbackground example input:\\nIntranet is an information system that integrates tasks inside an organization using Internet technology and communication protocols, and is mainly used by banks, securities, large companies, the government, local governments, schools, houses [1], public institutions, and the military. In other words, it is a 'in-house network' that can only be used by the organization, and can be accessed only inside the organization.Originally a network used by the U.S. Department of Defense for military purposes, it was the first to use a four-story initial TCP/IP method. Since then, due to the standardization of protocols, the cost has begun to decrease dramatically compared to the beginning, and the current Internet concept is to expand the system to the outside, not the inside, thanks to cost reduction. Conversely, the closed Internet is an intranet.In fact, the name intranet is a word that has been coined since the concept of the Internet was created, to distinguish networks used within a particular group for comparison. In other words, the first thing to be created was the intranet, but as the expanded object was used externally, it was first named the Internet, and the original was only later named to distinguish it from the expanded version.\\n\\n\\nStable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.  It was developed by the start-up Stability AI in collaboration with a number of academic researchers and non-profit organizations.\\nStable Diffusion is a latent diffusion model, a kind of deep generative neural network. Its code and model weights have been released publicly,  and it can run on most consumer hardware equipped with a modest GPU with at least 8 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services. \\nArchitecture :\\nStable Diffusion uses a kind of diffusion model (DM), called a latent diffusion model (LDM) developed by the CompVis group at LMU Munich. Introduced in 2015, diffusion models are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. Stable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder. The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image. Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion. The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space. The denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism. For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space. Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "background example input:\n",
    "Intranet is an information system that integrates tasks inside an organization using Internet technology and communication protocols, and is mainly used by banks, securities, large companies, the government, local governments, schools, houses [1], public institutions, and the military. In other words, it is a 'in-house network' that can only be used by the organization, and can be accessed only inside the organization.Originally a network used by the U.S. Department of Defense for military purposes, it was the first to use a four-story initial TCP/IP method. Since then, due to the standardization of protocols, the cost has begun to decrease dramatically compared to the beginning, and the current Internet concept is to expand the system to the outside, not the inside, thanks to cost reduction. Conversely, the closed Internet is an intranet.In fact, the name intranet is a word that has been coined since the concept of the Internet was created, to distinguish networks used within a particular group for comparison. In other words, the first thing to be created was the intranet, but as the expanded object was used externally, it was first named the Internet, and the original was only later named to distinguish it from the expanded version.\n",
    "\n",
    "\n",
    "Stable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.  It was developed by the start-up Stability AI in collaboration with a number of academic researchers and non-profit organizations.\n",
    "Stable Diffusion is a latent diffusion model, a kind of deep generative neural network. Its code and model weights have been released publicly,  and it can run on most consumer hardware equipped with a modest GPU with at least 8 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services. \n",
    "Architecture :\n",
    "Stable Diffusion uses a kind of diffusion model (DM), called a latent diffusion model (LDM) developed by the CompVis group at LMU Munich. Introduced in 2015, diffusion models are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. Stable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder. The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image. Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion. The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space. The denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism. For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space. Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971ea1ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global messages\n",
    "global problems\n",
    "problems = []\n",
    "def initial_setting():\n",
    "    region = 'Computer vision'\n",
    "    global messages\n",
    "    \n",
    "    query = \"You are a robot that makes questions. You are the user's study area,\\\n",
    "    question type (four-point multiple, five-point multiple, short-answer, multiple-answer, descriptive), \\\n",
    "    You need to collect other information that matches the difficulty level and context of the problem. \\\n",
    "    The prompt should contain all the information you have received. \\\n",
    "    You should ask the user questions until you are confident that you can generate the perfect prompt. \\\n",
    "    Your answers should be clearly formatted and optimized for ChatGPT interactions. \\\n",
    "    Your initial answer starts with asking about background knowledge. \\\n",
    "    Your answer will then begin by asking what type of problem you want, \\\n",
    "    the difficulty of the problem, and any additional information you need.\\\n",
    "    Now, Ask for background information first.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\" : \"system\",\n",
    "         \"content\" : f\"From now on, you are the problem maker related to the {region}.\"},\n",
    "        {\"role\" : \"user\",\n",
    "         \"content\" : query}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    messages.append({\"role\" : \"assistant\",\n",
    "                     \"content\" : answer})\n",
    "    return answer\n",
    "\n",
    "# 배경지식 입력 함수\n",
    "def set_background_knowledge(knowledge):\n",
    "    global messages\n",
    "    content = \"Background knowledge includes :\\n\"\n",
    "    content = content + knowledge\n",
    "    \n",
    "    messages.append({\"role\" : \"user\", \"content\" : content})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "\n",
    "    messages.append({\"role\" : \"assistant\",\n",
    "                     \"content\" : answer})\n",
    "    return answer\n",
    "    \n",
    "def interaction(problem_type_input, difficulty_input, additional_information_input):\n",
    "    global messages\n",
    "    content = \"Please make a problem based on the following. \\\n",
    "    After the question, Please tell me the answer to the question and its explanation.\\\n",
    "    translate it into Korean\\n\"\n",
    "    content = content + f'type of problem : {problem_type_input}\\n\\\n",
    "    difficulty of the problem : {difficulty_input}\\nadditional information : {additional_information_input}'\n",
    "    \n",
    "    messages.append({\"role\" : \"user\", \"content\" : content})\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    messages.append({\"role\" : \"assistant\",\n",
    "                     \"content\" : answer})\n",
    "    return answer\n",
    "\n",
    "def store_problem(question_output):\n",
    "    global problems\n",
    "    problems.append(question_output)\n",
    "\n",
    "def show_fn():\n",
    "    global problems\n",
    "    str_problems = '\\n=================================================================================================\\n'.join(problems)\n",
    "    return str_problems\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"ChatGPT generating problems\")\n",
    "    with gr.Tab(\"Question\"):\n",
    "        start_btn = gr.Button(\"Start\")\n",
    "\n",
    "        background_knowledge_input = gr.Textbox(label='Background Knowledge') \n",
    "        initial_btn = gr.Button(\"Background Knowledge\")\n",
    "\n",
    "        problem_type_input = gr.Dropdown([\"Easy\", \"Medium\", \"Hard\"] ,label='Difficulty')\n",
    "        difficulty_input = gr.Dropdown([\"four-point multiple\",\"five-point multiple\",\n",
    "                                        \"short-answer\",\"multiple-answer\",\"descriptive\"], label=\"Problem Type\")\n",
    "        additional_information_input = gr.Textbox(label='Additional Information')\n",
    "        with gr.Row():\n",
    "            btn = gr.Button(\"Go\")\n",
    "            store_btn = gr.Button(\"Store\")\n",
    "            \n",
    "        question_output = gr.Textbox(label=\"Output Box\")\n",
    "        \n",
    "    with gr.Tab(\"Answer\"):\n",
    "        show_btn = gr.Button(\"Show\")\n",
    "        output = gr.Textbox(label=\"Output Box\")\n",
    "        \n",
    "    \n",
    "    start_btn.click(fn = initial_setting, outputs = question_output)\n",
    "    initial_btn.click(fn = set_background_knowledge,inputs = background_knowledge_input, outputs = question_output)\n",
    "    btn.click(fn = interaction, inputs = [problem_type_input, difficulty_input, additional_information_input],\n",
    "              outputs = question_output)\n",
    "    store_btn.click(fn = store_problem, inputs = question_output)\n",
    "    show_btn.click(fn = show_fn, outputs = output)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a27bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ef8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
